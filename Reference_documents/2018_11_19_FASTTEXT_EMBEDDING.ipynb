{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以我們這組專題題目Avito中的title為例：\n",
    "\n",
    "MAX_NUM_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 15\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "#前面同之前的流程，建立 token 字典（此例字典大小為50000）, fit後轉換為數字list再截長補短（此例長度為50）\n",
    "tokenizer = text.Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "sequence = tokenizer.texts_to_sequences(df.title.str.lower())\n",
    "pad_sequence=pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先至fasttext官網下載所需語言的詞向量https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "#範例為俄語的cc.ru.300.vec向量檔案\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('cc.ru.300.vec', encoding = 'utf8'))\n",
    "\n",
    "#word_index ： 表示從語料庫之中保留多少個單詞。 因為Keras需要預留一個全零層， 所以+1\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "\n",
    "#構建一個大小為[nb_words, EMBEDDING_DIM]的矩陣\n",
    "#遍歷word_index。 將word在Fasttext模型之中對應vector複製過來\n",
    "#排列順序按照Tokenizer在fit之後的詞順序，建立權重embedding_matrix之後留做稍後Embedding層使用\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "# words not found in embedding index will be all-zeros.\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras 模型寫法：\n",
    "#trainable = False，並將embedding權重指定為剛剛構建的embedding矩陣embedding_matrix。（weights=[embedding_matrix]）\n",
    "seq_title_description = Input(shape=[15], name=\"seq_title_description\")\n",
    "emb_seq_title_description = Embedding(nb_words, EMBEDDING_DIM, weights = [embedding_matrix], trainable = False)(seq_title_description)\n",
    "\n",
    "\n",
    "#其他程式寫法同一般keras模型\n",
    "model = Model([seq_title_description], output)\n",
    "model.compile(optimizer = 'adam',\n",
    "                  loss= root_mean_squared_error,\n",
    "                  metrics = [root_mean_squared_error])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
